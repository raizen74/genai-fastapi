curl -G "http://localhost:8000/generate/text" --data-urlencode "prompt=How do I create a FastAPI endpoint?"

uv run streamlit run ./streamlit/client_text.py
uv run streamlit run ./streamlit/client_audio.py
uv run streamlit run ./streamlit/client_image.py

uv run bentoml serve building_genai_services.bentoml.bento:Generate

curl localhost:8000/generate/text \
  -H "Content-Type: application/json" \
  -d '{"model":"tinyLlama","prompt":"Write a friendly 2-sentence introduction to a newsletter about AI.","temperature":0.2}'

uv run python -m vllm.entrypoints.openai.api_server \
--model "TinyLlama/TinyLlama-1.1B-Chat-v1.0" \
--dtype "auto" \
--gpu-memory-utilization 0.85 \
--swap-space 1 \
--tensor-parallel-size 1 \
--api-key "secret1234"

docker run -p 6333:6333 -p 6334:6334 \
-v $(pwd)/qdrant_storage:/qdrant/storage:z \
qdrant/qdrant

uv run streamlit run ./streamlit/client_upload.py

docker run -p 5432:5432  \
	-e POSTGRES_USER=fastapi \
	-e POSTGRES_PASSWORD=mysecretpassword \
	-e POSTGRES_DB=backend_db \
	-e PGDATA=/var/lib/postgresql/data \
  -v "$(pwd)"/dbstorage:/var/lib/postgresql/data \
  postgres:latest

Test the create/conversations and store/messages endpoints:

First create a conversation:

curl -X POST http://localhost:8000/conversations \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Test Conversation",
    "model_type": "gemma2b"
  }'

Store a message in the conversation:

curl -X POST -G "http://localhost:8000/store/message/1" \
  --data-urlencode "prompt=Write a friendly 2-sentence introduction to a newsletter about AI."

alembic init alembic
alembic revision --autogenerate -m "Initial migration" - Create a new migration based on model changes
alembic upgrade head - Apply all pending migrations
alembic downgrade -1 - Rollback the last migration
alembic current - Show current migration version
alembic history - Show migration history

uv run pytest --asyncio-mode=auto